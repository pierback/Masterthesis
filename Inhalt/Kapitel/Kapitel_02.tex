\chapter{Stand der Technik}
\label{kap:Kapitel02}
%\nocite{*} << blendet alle Referenzen ein, auch wenn diese nicht im Text zitiert wurden
%
       
%
\section{Blockchain}
Der folgende Abschnitt basiert auf den Inhalten dieser Literatur: \ref{Antonopoulos:2017:MBP:3164842}, \ref{BitcoinEthNCo}, \ref{BchainPracticalGuide} und \ref{MasteringBlockchain}. 

\subsection{Funktionsweise \& Konzepte}
Damit das Verständnis für die Blockchain Technologie geschaffen werden kann, sollte zuerst die darunter liegende Technologie betrachtet werden. 
So wird der Begriff der Blockchain oft auch mit dem des \textit{distributed ledger} gleichgesetzt. Ein \textit{distributed ledger} ist eine \textit{verteilte Datenbank}, welche im Kontext einer Blockchain auch als \quotes{verteiltes Konto} bzw. \quotes{dezentral geführtes Kontobuch} \cite{DL:bafin} verstanden wird, in welchem jegliche Transaktionen abgespeichert werden.\\
Allerdings ist diese Gleichsetzung nicht ganz korrekt, denn eine Blockchain ist nur ein spezieller Typus eines \textit{distributed ledgers}. So kann nämlichh dieser neben Transaktionen auch aus weiteren Daten bestehen, wohingegen bei einer Blockchain die Blöcke stets Transaktionen beinhalten. \\
Somit stellt ein \textit{distributed ledger} die techologische Grundlage aller virtuellen Währungen dar und ist dadurch auch einer der Hauptgründe weshalb Kryptowährungen auf einer Blockchain basieren.\\\\
Grundsätzlich steht Blockchain für eine Verkettung von geordneten Blöcken, welche in sich eine oder mehrere Transaktionen beinhalten. 
Zu den Transaktionsdaten wird zudem ein Hashwert des Vorgängerblocks und ein Zeitstempel mit im Block abgespeichert. Erst aufgrund der Berücksichtigung des Hashwerts des vorherigen Blocks werden die Blöcke miteinander verknüpft und bilden somit eine chronologisch geordnete Kette.\\
Eine weitere Beschaffenheit einer Blockchain ist die Dezentralität durch das aufgespannte \textit{Peer-to-Peer} Netzwerk. Der Vorteil dieser Topologie besteht in der Vakanz eines zentralen Knotens, welcher für das Abspeichern und Bereitstellen aller bestehenden Daten eines Netzwerks zuständig ist. Denn jeder Knoten in einem solchem Netzwerk ist sowohl Sender als auch Empfänger, sodass jeder Teilnehmer eine Kopie des Datenbestandes besitzt, was einen \quotes{Single Point of Failure} völlig ausschließt. Aus diesem Grund sind Daten einer Blockchain nie nur bei einem Knoten abgespeichert, sondern jede Node besitzt eine Kopie der aktuellen Blockchain.\\
Welche Daten schließich auf die Blockchain geschrieben werden dürfen oder genauer gesagt ob eine Transaktion durchgeführt werden darf, erfolgt stets in abetracht des Konsens aller Parteien im Netzwerk. Diese Eigenschaft wird als \textit{distributed consensus} bezeichnet und ist das Fundament einer jeder Blockchain. Durch die Einbeziehung eines jeden Teilnehmers in der Entscheidungsfindung wird die Notwendigkeit einer zentralen Entscheidungsinstanz obsolet. \\
Damit löst die Blockchain ein Gedankenexperiment von Lamport aus dem Jahr 1982. Es plant eine Gruppe an byzantinischen Generälen, welche jeweils verschiedene Teile der byzantinischen Armee befehligen, einen Angriff auf eine Stadt, der nur in einem Sieg resultiert wenn alle gemeinsam angreifen. Der einzige Weg der Interkommunikation und Koordination ist via Boten. Somit entsteht eine Problematik, sollten sich ein oder mehrere Verräter unter den Generälen befinden, die falsche Informationen streuen.\\
Aus diesem Grund bedarf es einem Mechanismus der es erlaubt, trotz Falschinformationen durch potentielle Verräter, einen Konsens unter den Generälen zu schaffen, um gemeinsam anzugreifen. \\
Analog zur Blockchain entsprechen die Generäle den Knoten im Netzwerk, die Verräter sind \quotes{bösartige} (malicious) Knoten und die Boten sind die Kommunikationskanäle zwischen den Knoten.\\
Gelöst wurde dieses Problematik durch die Publikation des \textit{Practical Byzantine Fault Tolerance (PBFT)} Algorithmus (1999, Castro und Liskov), welcher nach einer bestimmten Anzahl an Nachrichten mit gleichem signierten Inhalt einen Konsens unter allen Knoten herstellt.   
Der Mechanismus welcher dafür zuständig ist diesen Konsens herbeizuführen, ist je nach Implementierung des Blockchainprotokolls unterschiedlich. Eine beliebte Methodik, welche unteranderem von Bitcoin und Ethereum verwendet wird, ist der  \textit{Proof-of-Work} Ansatz. Hierbei werden von den sog. \quotes{Minern} Iterationen an aufwändigen und komplexen Berechnungen durchgeführt, die sicherstellen sollen, dass die benötigten kryptographischen Berechnungen für eine Transaktion durchgeführt und die Daten einer Transaktion validiert werden.\\
Eine weitere Besonderheit einer Blockchain ist die Unveränderbarkeit der darauf gespeicherten Daten. So gibt es, im Gegensatz zu den bekannten CRUD-Operationen \footnote{CRUD:Wiki}, welche zur Kommunikation zwischen Client und Server verwendetet werden, um Daten zu schreiben, zu downloaden, zu löschen und zu editieren, bei einer Blockchain lediglich eine Schreib- und eine Leseoperation. Weswegen im Zusammenspiel mit dem Konsensverfahren Transaktionen auf einer Blockchain einzig hinzugefügt und gelesen, jedoch nie zu einem späteren Zeitpunkt gelöscht oder editiert werden können. \\
Dabei sind die gespeicherten Daten (unverschlüsselt oder auch verschlüsselt) für alle im Netzwerk einsehbar und bedeutet somit volle Transparenz für jeden User. \\ 
Die gerade geschilderten Eigenschaften sind einer jeden Blockchain inhärent bzw. in einer abgeänderten Form (z.B. \textit{Proof-of-Stake} \footnote{Konsensalgorithmus welcher anstatt von Rechenleistung einen bestimmten Betrag an Ether als Pfand erwartet. Je höher dieser Wert desto warscheinlich ist es, dass der Nutzer die Transaktion als neuen Block bestätigt \cite{PoS:BTCE,PoS:Wiki}} anstatt \textit{Proof-of-Work}) vorhanden. 
Was jedoch erst in neueren Blockchains (Blockchain 2.0) vorzufinden ist, sind die \textit{Smart Contracts}.\\
Smart Contracts sind Programme welche auf einer Blockchain installiert werden können.\\
Diese Programme können z.B. Business Logiken abbilden und ausführen oder auch Verpflichtungen und Vereinbarungen im rechtlichen Sinne durchsetzten, ohne der Notwendigkeit eines Mittelmanns, welcher das Vertrauen aller beteiligten Parteien inne hat. Diese \quotes{Trust-Komponente} wird durch die Anerkennung aller Parteien von dem Smart Contract übernommen. \\
Das erste mal in Erscheinung getreten sind Smart Contracts mit der Veröffentlichung der Ethereum Blockchain, welche nun im Anschluss genauer betrachtet wird.

\subsection{Ethereum}
Die Ethereum Blockchain wurde 2015 in Betrieb genommen, mit dem Ziel nicht nur eine Kryptowährung zu schaffen, sondern eine Plattform zu entwickeln auf welcher sog. Dapps (Decentralized Apps) betrieben werden können.
So wird Ethereum im Vergleich zu Bitcoin auch als Blockchain 2.0 bezeichnet, da es eben nicht nur eine Kryptowährung umfasst, sondern es aufgrund der Smart Contracts es möglich ist Software auf einer Blockchain zu installieren. \\
Als Ethereum wird genauer genommen das Protokoll tituliert welches die Blockchain implentiert. Die Kryptowährung die auf der Blockchain basiert wird als \textit{Ether} bezeichnet und fungiert zudem als Zahlungsmittel im Kontext der Smart Contracts. So ist bzw. war das Bestreber der Gründer nicht eine weitere Kryptowährung zu schaffen, sondern einen Art \quotes{Supercomputer}, welcher immer online ist und aus Milionen von Computern im Netzwerk besteht, die ihre Rechenleistung zur Verfügung stellen und als Gegenleistung bzw. Anreiz in Form von Ether vergütet werden. \\
Dabei können zwar, wie bei einer Kryptowährung, Transaktionen durchgeführt und Ether von einem Konto auf ein anderes transferiert werden. Jedoch liegt der eigentliche Fokus auf den Smart Contracts und den Dapps.\\
Dazu wurde die EVM (Ethereum Virtual Machine) entwickelt, in welcher letztlich die Smart Contracts bzw. Dapps gehostet und ausgeführt werden. Dabei stellt die Virtual Machine eine Abstraktionsebene zur physischen Schicht der Blockchain dar und ermöglicht es dadurch den Smart Contracts Daten auf die Blockchain zu speichern und bietet gleichzeitig eine Laufzeitumgebung in der die Anwendungen ausgeführt werden können.\\
Die Implementierung der Smart Contracts erfolgt stets in der eigens entwickelten Programmiersprache Solidity. Diese folgt dem Prinzip der Objekt Orientierung, ist der Sprache Javascript angelehnt und ist zudem Turing-Vollständig, was im Bezug auf die Entwicklung von Apps und eine hohe Bandbreite an Anwendungsfällen eröffnet. \\
Damit die Funktionen der Smart Contracts überhaupt genutzt werden können, muss einem jedem Methodenaufruf bzw. jeder Transaktion Ether - im Kontext der Smart Contracts als \quotes{Gas} bezeichnet - mitgegeben werden, um die Miner für ihre zur Verfügung gestellte Rechenleistung zu entlohnen. Das bedeutet um Daten auf die Blockchain zu speichern, wird je nach Transaktion eine bestimmte, Menge an Gas benötigt, welches im Endeffekt den Minern als Anreiz und Belohnung zur Bereitstellung von Rechenleistung übertragen wird. \\
Wie das Mining bereits impliziert, basiert der Konsensalgorithmus der Ethereum Blockchain auf dem Proof of Work Konzept. Da dieser Ansatz jedoch einige Nachteile mit sich bringt, was Energieeffenzienz und Transaktionen pro Sekunde betrifft, wird voraussichtlich im Juni 2019 der Wechsel auf einen Proof of Stake Konsensalgorithmus erfolgen. \\\\
Die große Community, der Opensource Ansatz, die Verfügkbarkeit von Libraries in verschiedenen Programmiersprachen und Kommandzeilenanwendungen, sowie die Möglichkeit Smart Contracts bzw. Dapps auf der Blockchain zu betreiben, sind eine der Hauptgründe, weshalb die Wahl bei dieser Arbeit auf Ethereum gefallen ist.   
\cite{BitcoinEthNCo,DL:bafin,Antonopoulos:2017:MBP:3164842}


\section{Machine Learning}
\subsection{"Uberblick}

Der Fortschritt in der Entwicklung von Machine Learning Algorithmen in den letzten Jahren haben dazu geführt, dass Algorithmen in vielerlei Bereiche des Alltags Einzug gefunden haben. Vor allem bei Aufgaben bei denen eine große Menge an Daten an vorhanden ist, werden diese Algorithmen verwendet, um relevante Informationen daraus zu extrahieren. \\
Mit dem Anstieg der Digitalisierung entstehen immer mehr Datensätze von solch enormer Größe. Sei es durch die Vernetzung von Geräten und Sensoren im Bereich des IoT oder auch das Sammeln von Nutzerdaten durch Smartphone-Apps, die Intention besteht stets in der Erlangung eines besseren Verständnis für eine konkrete Problemstellungen in der realen Welt. \\
Aufgrund dessen ergeben sich eine Vielzahl an Anwendungsfelder für Machine Learning Algorithmen.\\
Eines davon fällt unter die Begrifflichkeit Data Mining. Hierbei erfolgt eine Entscheidungsfindung durch den Algorithmus anhand von Datenaufzeichnung in einem bestimmten Gebiet. Als Beispiel wäre hier die Analyse von Krankenakten zu nennen, welche zu einem besseren Verständnis von Krankheitsbildern, sowie einer genaueren Diagnose führen soll. \\
Ein weiteres Anwendungsgebiet sind Problemstellungen, deren Lösung nicht durch eine (klassisch) imperativ implementierte Software gefunden werden kann.
Dies ist dann der Fall wenn die Anwendung aufgrund deren enormen Vielzahl nicht alle möglichen Status abbilden kann. Zu nennen sind hier die Spracherkennung oder auch das autonome Fahren, welche schlichtweg nur mit Machine Learning realisierbar sind.\\
In die Kategorie des nächsten Gebietes, fällt auch die Problemstellung dieser Arbeit. Es handelt sich um die Personalisierung (\quotes{customization}) von Software-Anwendungen und Services. Dabei wird Machine Learning dazu verwendet, um das Verhalten der User besser zu verstehen. Die gesammelten Nutzerdaten werden analysiert und Präferenzen der User aufgrund ihres Online-Verhaltens abgeleitet. So werden zum Beispiel im Falle von Facebook oder Google, dadurch gezielt Werbeanzeigen platziert und auf den User abgestimmt.\cite{mlmitchel} \\\\
Somit eignet sich sich je nach Anwendungsgebiet und Problemstellung unterschiedliche Typen von Machine Learning Algorithmen. 
Eine inhärente Eigenschaft eines Machine Learning Algorithmus ist die Notwendigkeit eines Input \textit{Y} in Form von Features. Ein Feature ist eine numerische Repräsentation eines Teilaspekts der Inputdaten. Dabei ist die Konkatenation dieser auch als Feature Vektor bekannt. Machine Learning beschreibt somit die Identifikation von Strukturen und Muster auf Basis einer Kollektion von Feature Vektoren. \cite{fteng} \\\\
Generell lässt sich eine Unterteilung der Machine Learning Algorithmen in drei Typen machen. Dabei erfolgt eine Unterscheidung aufgrund Art des Lernprozesses. \\
Die wohl bekannteste Kategorie ist das Supervised Learning. Algorithmen welche unter diesen Teilbereich fallen, haben das Ziel für einen Input X einen Output Y zu finden. Hierfür wird diesen in der Lernphase ein Datensatz vorgelegt, bei welchem diese Abbildung (X $\rightarrow$ Y) bereits vorhanden ist und die Algorithmen sich somit Wissen aneignen können. Nach dem erfolgreichen Durchlauf der Lernphase sind die Algorithmen dazu in der Lage bei Eingabe von unbekannten Input-Daten, basierend auf dem angeeigneten Wissen, eine Generalisierung durchzuführen und einen Output Y (Labels) für X zu liefern. \\
Der Nachteil dieser Algorithmen liegt in der Aufbereitung und Bereitstellung dieser Lerndaten, welche in der Regel manuell erstellt werden müssen. \cite{Engemann}\\\\
Im Kontrast dazu ist das Unsupervised Learning zu verstehen. Diese Algorithmen benötigen zum Lernen nur einen Input X im Datensatz. Denn diese können trotz fehlender Labels im Datensatz statistische Strukturen aufdecken bzw. ein Modell für diesen Datensatz erarbeitet. Beispiele hierfür sind das Clustering (k-Means) der Daten oder die Reduktion (PCA) zu nennen. \cite{Engemann,mlmitchel, Sutton}\\\\
Ein weiterer Lernansatz welcher sich zwischen den beiden eben genannten Kategorien einreiht, wird als Reinforcement Learning bezeichnet und wird im nächsten Unterkapitel genauer betrachtet.

\subsection{Reinforcement-Learning}
\label{subsec:rl}
Das Grundprinzip des Reinforcement-Learning besteht darin bestimmte Situationen auf Aktionen zu projizieren, um dabei den numerischen Reward des Agenten zu maximieren. Diese Aktionen werden dem Agenten jedoch nicht durch eine \quotes{Superviser-Instanz} mitgeteilt, sondern dieser versucht durch die \quotes{Trial and Error} Methodik herauszufinden, welche Aktion in welchem Zustand  den größten Reward zur Folge hat. Dabei können diese Aktionen nicht nur die Belohnung des Agenten beeinflussen, sondern zudem die Umgebung in der er sich bewegt und dadurch auch den Folgezustand. \\
Dieses Konzept der Reward-Maximierung resultiert in dem Tradeoff zwischen \textit{Exploration} und \textit{Exploitation}. \textit{Exploration} beschreibt den Versuch mehr Information über die Umgebung zu erlangen, indem die Reward-Maximierung außer Acht gelassen und eine Aktion zufällig ausgewählt wird, mit dem Ziel den bisherigen Reward zu übertreffen. Im Gegenteil dazu spezifiziert \textit{Exploitation} die Maximierung des Rewards, indes der Agent stets auf die, in einem bestimmten Zustand, bestbewertete Aktion zurückgreift.
Je nach Algorithmus und Lernproblem variiert das Verhältnis der beiden, welches durch eine (iterative) Justierung der Parameter anzupassen gilt. \\
Durch die Wechselwirkung der beiden ist es die Aufgabe des Agenten eine Strategie zu finden die letztlich den maximalen Reward garantiert, indem es sein Verhalten in den jeweiligen Zuständen bereits vorgibt. \\
Dabei gilt es vor allem zu beachten, ob es sich bei dem Lernproblem um ein deterministisches oder nichtdeterministisches handelt. Deterministisch bedeutet, es wird stets die gleiche Aktion in einem bestimmten Zustand gewählt, wohingegen nichtdeterministisch lediglich eine Wahrscheinlichkeitsverteilung beschreibt, anhand der Agent in einem Zustand entscheidet, welche Aktion auszuwählen ist. \\\\
Im Allgemeinen lassen sich folgende inhärente Eigenschaften an Reinforcement Learning Algorithmen feststellen:
\begin{itemize}
    \item Agent: lernendes System, dessen Ziel es ist seine Belohnung zu maximieren, indem es seine Umgebung wahrnimmt und mittels Aktionen beeinflusst
    \item Umgebung: diese beschreibt Bestandteile außerhalb des lernenden Systems (andere Agenten, Programme, Menschen etc.) und ist entweder von deterministischer oder nichtdeterministischer Natur
    \item Reward: numerischer Wert der die Nützlichkeit einer Aktion im aktuellen Zustand beschreibt und dadurch als direktes Feedback für den Agenten zu sehen ist
    \item Wertfunktion: ist die Akkumulation aller erhaltenen Belohnungen, startend bei einem bestimmten Zustand bis hin zum Ende des Experiments und stellt somit eine Schätzung bezüglich der Wertigkeit dieses Zustandes dar
    \item Strategie: definiert das Verhalten des Agenten zu einem bestimmten Zeitpunkt bzw. welche Aktionen in welchen Zuständen ausgeführt wird
    \item Umgebungsmodell: ist die Abbildung des Verhaltens der Umgebung, um kommende Zustände und Belohnung vorherzusagen und auf dieser Basis eine geeignete Strategie zu finden
\end{itemize}
Jedoch sind nicht alle Reinforcement Algorithmen gleich, sondern in ihrer Definition und Umsetzung der eben genannten Eigenschaften (sehr) unterschiedlich. Eine Kategorisierung wird in der Regel anhand der Berechnung der Wertfunktion gemacht und dadurch ergeben sich drei verschiedene Klassen an Reinforcement Algorithmen. Diese differieren durch verschiedene Lösungsansätze in der Berechnung der Wertfunktion. \\\\
Als erste Kategorie ist die Dynamische Programmierung (DP) zu nennen.
Diese Algorithmen sind mathematisch präzise, benötigen dafür aber ein perfektes Umgebungsmodell, welches bereits die Rewards und Übergangswahrscheinlichkeiten aller Zustände beinhaltet. So beruht die Berechnung der Wertfunktion auf dem Markow-Entscheidungsproblem (MEP oder auch MDP \textit{Markov decision process}). Dieses Modell erlaubt die Erarbeitung einer Strategie zur Reward-Maximierung, unter der Berücksichtigung der bekannten Übergangswahrscheinlichkeiten und den erwarteten Rewards. 
Das ist auch der große Nachteile dieser Methodik, da diese Berechnungen sehr rechenaufwändig sind und zudem ein perfektes Umgebungsmodell in den meisten Fällen nicht vorhanden ist.\\\\
Unter die nächste Kategorie fallen die Monte Carlo (MC) Methoden. 
Im Gegensatz zur Dynamische Programmierung berechnen diese lediglich eine Schätzung der Wert-Funktion und finden aufgrund dessen die optimale Strategie. Außerdem benötigen diese auch kein Vorwissen über die Umgebung, sondern lernen durch die Interaktion mit der Umgebung, die Abfolge von Zuständen und Aktionen, sowie deren Rewards. Bei der Berechnung der Wertfunktion wird stets bis zum Ende einer Episode gewartet und eine Mittelung über alle erhaltenen Rewards durchgeführt. Dies funktioniert jedoch nur bei Problemstellungen bei denen Episoden auch terminieren. Somit ist es auch ein Nachteil der MC Methoden, da eine Bewertung eines Zustandes immer erst am Ende einer Episode realisiert werden kann.\\\\
Der letzte Ansatz wird als Temporal Difference (TD) Learning bezeichnet. 
Hierbei wird im Kontrast zu den MC Methoden nicht erst auf die Beendigung der Episode gewartet, sondern die Wertfunktion eines Zustandes gleich im nächsten Zeitschritt geschätzt. Diese Schätzungen basieren wiederum auf den bereits vorangegangene Schätzungen und nähert sich dadurch inkrementell dem \quotes{wahren} Wert der Wertfunktion an, ohne dabei jedes mal auf die Terminierung einer Episode warten zu müssen. So ist auch bei dieser Art von Algorithmen kein Umgebungsmodell von Nöten, sondern die Bewertungen der Aktionen bzw. werden über die Zeit hinweg erörtert und gesammelt.

